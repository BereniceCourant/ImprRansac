{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#import tensorflow as tf\n",
    "#import keras\n",
    "#from keras import backend as K\n",
    "#from keras.datasets import mnist\n",
    "#from keras.models import Sequential, Model\n",
    "#from keras.layers import Dense, Dropout, Flatten\n",
    "#from keras.layers import Conv2D, MaxPooling2D, Input\n",
    "#from keras import activations\n",
    "#import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from matplotlib import pyplot as plt\n",
    "from math import *\n",
    "from itertools import chain\n",
    "\n",
    "import import_ipynb\n",
    "from Utils import *\n",
    "from Model import *\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_noise = [2, 2]\n",
    "image_size = (400, 300)\n",
    "nbr_inliers=300\n",
    "nbr_outliers=200\n",
    "size_patch = (8, 8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_dim = size_patch[0]*2+1\n",
    "model = Weights().to(DEVICE)\n",
    "model_old_loss = Weights().to(DEVICE)\n",
    "\n",
    "model_patch = Weights2().to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(y1, y2):\n",
    "    n = len(y1[0])\n",
    "    s = 0\n",
    "    for k in range(n):\n",
    "        print(y1[k][0], y2[k][0])\n",
    "        s += abs(y1[k][0]-y2[k][0])\n",
    "    return s\n",
    "\n",
    "def train_model(model, image, sigma, nbr, nbr_per_line, epochs, batch_size, nbr_inliers, nbr_outliers, loss = 1):\n",
    "    ((x_test, y_test), (x_train, y_train)) = split_training(generate_training_data(image, sigma, nbr, nbr_per_line, nbr_inliers, nbr_outliers, loss = loss))\n",
    "    #x_test, y_test = x_test.to(DEVICE), y_test.to(DEVICE)\n",
    "    #x_train, y_train = x_train.to(DEVICE), y_train.to(DEVICE)\n",
    "    #print(x_test, y_test)\n",
    "    #print(x_train, y_train)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=10**(-5))\n",
    "    print(\"data generated\")\n",
    "    \n",
    "    # Train step\n",
    "    train_ds = TensorDataset(x_train, y_train)\n",
    "    train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    print(len(train_dl))\n",
    "    \n",
    "    valid_ds = TensorDataset(x_test, y_test)\n",
    "    valid_dl = DataLoader(valid_ds, batch_size=batch_size, shuffle=True)\n",
    "    m_train = 0\n",
    "    for k in range(epochs):\n",
    "        loss_train = 0\n",
    "        for xb, yb in train_dl:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            #print(xb.shape)\n",
    "            #print(yb.shape)\n",
    "            model.train()  # <-- here\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_predict = model(xb)\n",
    "            #print(xb.shape)\n",
    "            loss = loss_fn(y_predict, yb)\n",
    "            loss_train += loss.item()\n",
    "            m_train += len(yb)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Eval\n",
    "        model.eval()  # <-- here\n",
    "        with torch.no_grad():\n",
    "            loss_eval = sum(loss_fn(model(xb.to(DEVICE)), yb.to(DEVICE)) for xb, yb in valid_dl)\n",
    "            m_test = sum(len(yb) for xb, yb in valid_dl)\n",
    "\n",
    "        print(\"iteration = \", k+1, \" ; loss_train = \", loss_train, \" ; loss_eval = \", loss_eval.item())\n",
    "        \n",
    "        if k == epochs - 1:\n",
    "            last_weight = model(x_test.to(DEVICE))\n",
    "            print(last_weight)\n",
    "            print(y_test)\n",
    "            print(sum(abs(last_weight[i][0]-y_test[i][0]) for i in range(len(y_test)))/len(y_test))\n",
    "            \n",
    "\n",
    "def train_model_patch(model, image, sigma, nbr, nbr_per_line, epochs, batch_size, nbr_inliers, nbr_outliers):\n",
    "    ((x_test, y_test), (x_train, y_train)) = split_training_bis(generate_training_data_patch(image, sigma, nbr, nbr_per_line, nbr_inliers, nbr_outliers))\n",
    "    #x_test, y_test = x_test.to(DEVICE), y_test.to(DEVICE)\n",
    "    #x_train, y_train = x_train.to(DEVICE), y_train.to(DEVICE)\n",
    "    #print(x_test, y_test)\n",
    "    #print(x_train, y_train)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=10**(-6))\n",
    "    print(\"data generated\")\n",
    "    \n",
    "    # Train step\n",
    "    x_train1, x_train2, x_train3 = x_train\n",
    "    train_ds = TensorDataset(x_train1, x_train2, x_train3, y_train)\n",
    "    train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    print(len(train_dl))\n",
    "    \n",
    "    x_test1, x_test2, x_test3 = x_test\n",
    "    valid_ds = TensorDataset(x_test1, x_test2, x_test3, y_test)\n",
    "    valid_dl = DataLoader(valid_ds, batch_size=batch_size, shuffle=True)\n",
    "    m_train = 0\n",
    "    for k in range(epochs):\n",
    "        loss_train = 0\n",
    "        for xb1, xb2, xb3, yb in train_dl:\n",
    "            xb1, xb2, xb3, yb = xb1.to(DEVICE).float(), xb2.to(DEVICE).float(), xb3.to(DEVICE).float(), yb.to(DEVICE).float()\n",
    "            #print(xb.shape)\n",
    "            #print(yb.shape)\n",
    "            model.train()  # <-- here\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_predict = model((xb1, xb2, xb3))\n",
    "            #print(xb.shape)\n",
    "            loss = loss_fn(y_predict, yb)\n",
    "            loss_train += loss.item()\n",
    "            m_train += len(yb)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Eval\n",
    "        model.eval()  # <-- here\n",
    "        with torch.no_grad():\n",
    "            loss_eval = 0\n",
    "            m_test = 0\n",
    "            for xb1, xb2, xb3, yb in valid_dl:\n",
    "                predict_test = model((xb1.to(DEVICE).float(), xb2.to(DEVICE).float(), xb3.to(DEVICE).float()))\n",
    "                loss_eval += loss_fn(predict_test, yb.to(DEVICE).float())\n",
    "                m_test += len(yb)\n",
    "\n",
    "        print(\"iteration = \", k+1, \" ; loss_train = \", loss_train, \" ; loss_eval = \", loss_eval.item())\n",
    "        \n",
    "        if k == epochs - 1:\n",
    "            last_weight = model((x_test1.to(DEVICE).float(), x_test2.to(DEVICE).float(), x_test3.to(DEVICE).float()))\n",
    "            print(last_weight)\n",
    "            print(y_test)\n",
    "            print(sum(abs(last_weight[i][0]-y_test[i][0]) for i in range(len(y_test)))/len(y_test))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data generated\n",
      "143\n",
      "iteration =  1  ; loss_train =  355.00507378578186  ; loss_eval =  9.349258422851562\n",
      "iteration =  2  ; loss_train =  338.76170629262924  ; loss_eval =  9.382851600646973\n",
      "iteration =  3  ; loss_train =  337.50466588139534  ; loss_eval =  9.23510456085205\n",
      "iteration =  4  ; loss_train =  334.458599075675  ; loss_eval =  9.279936790466309\n",
      "iteration =  5  ; loss_train =  331.6546429693699  ; loss_eval =  9.405237197875977\n",
      "iteration =  6  ; loss_train =  333.35048058629036  ; loss_eval =  9.598386764526367\n",
      "iteration =  7  ; loss_train =  327.62079095840454  ; loss_eval =  9.392875671386719\n",
      "iteration =  8  ; loss_train =  330.6391562446952  ; loss_eval =  9.2744140625\n",
      "iteration =  9  ; loss_train =  321.8542483821511  ; loss_eval =  9.526289939880371\n",
      "iteration =  10  ; loss_train =  320.8820285499096  ; loss_eval =  9.300515174865723\n",
      "iteration =  11  ; loss_train =  320.44209772348404  ; loss_eval =  9.49420166015625\n",
      "iteration =  12  ; loss_train =  314.69576148688793  ; loss_eval =  9.337041854858398\n",
      "iteration =  13  ; loss_train =  319.6184279769659  ; loss_eval =  9.594514846801758\n",
      "iteration =  14  ; loss_train =  311.1433520168066  ; loss_eval =  9.449992179870605\n",
      "iteration =  15  ; loss_train =  308.6388033926487  ; loss_eval =  9.214811325073242\n",
      "iteration =  16  ; loss_train =  306.1241799592972  ; loss_eval =  9.225286483764648\n",
      "iteration =  17  ; loss_train =  311.4488473832607  ; loss_eval =  9.403043746948242\n",
      "iteration =  18  ; loss_train =  307.0179592370987  ; loss_eval =  9.397193908691406\n",
      "iteration =  19  ; loss_train =  304.50995814055204  ; loss_eval =  10.183572769165039\n",
      "iteration =  20  ; loss_train =  302.880432844162  ; loss_eval =  9.855533599853516\n",
      "iteration =  21  ; loss_train =  302.5560909733176  ; loss_eval =  9.647366523742676\n",
      "iteration =  22  ; loss_train =  296.94283977150917  ; loss_eval =  9.841425895690918\n",
      "iteration =  23  ; loss_train =  302.09885144233704  ; loss_eval =  11.370850563049316\n",
      "iteration =  24  ; loss_train =  300.3488033115864  ; loss_eval =  9.753985404968262\n",
      "iteration =  25  ; loss_train =  292.9314574599266  ; loss_eval =  10.68716812133789\n",
      "iteration =  26  ; loss_train =  296.2741535604  ; loss_eval =  10.041665077209473\n",
      "iteration =  27  ; loss_train =  295.24152870476246  ; loss_eval =  12.021759033203125\n",
      "iteration =  28  ; loss_train =  292.2808677107096  ; loss_eval =  10.854965209960938\n",
      "iteration =  29  ; loss_train =  288.86300697922707  ; loss_eval =  10.943599700927734\n",
      "iteration =  30  ; loss_train =  283.1014891862869  ; loss_eval =  11.519845962524414\n",
      "iteration =  31  ; loss_train =  281.7344787865877  ; loss_eval =  11.016773223876953\n",
      "iteration =  32  ; loss_train =  283.32553392648697  ; loss_eval =  10.600767135620117\n",
      "iteration =  33  ; loss_train =  278.677262455225  ; loss_eval =  11.036462783813477\n",
      "iteration =  34  ; loss_train =  291.51860692352057  ; loss_eval =  12.675439834594727\n",
      "iteration =  35  ; loss_train =  279.9255574643612  ; loss_eval =  13.30537223815918\n",
      "iteration =  36  ; loss_train =  277.01430501043797  ; loss_eval =  11.518415451049805\n",
      "iteration =  37  ; loss_train =  276.41658943891525  ; loss_eval =  12.526046752929688\n",
      "iteration =  38  ; loss_train =  270.5432736054063  ; loss_eval =  12.565250396728516\n",
      "iteration =  39  ; loss_train =  283.0346977710724  ; loss_eval =  10.151983261108398\n",
      "iteration =  40  ; loss_train =  276.13627357780933  ; loss_eval =  11.210469245910645\n",
      "iteration =  41  ; loss_train =  269.971907556057  ; loss_eval =  11.248287200927734\n",
      "iteration =  42  ; loss_train =  270.15449337661266  ; loss_eval =  11.718267440795898\n",
      "iteration =  43  ; loss_train =  268.0002910345793  ; loss_eval =  11.810415267944336\n",
      "iteration =  44  ; loss_train =  265.8153532296419  ; loss_eval =  14.14063549041748\n",
      "iteration =  45  ; loss_train =  261.2552300989628  ; loss_eval =  13.990713119506836\n",
      "iteration =  46  ; loss_train =  256.5908058732748  ; loss_eval =  10.089269638061523\n",
      "iteration =  47  ; loss_train =  252.5015119165182  ; loss_eval =  14.3896484375\n",
      "iteration =  48  ; loss_train =  256.73724097013474  ; loss_eval =  13.812164306640625\n",
      "iteration =  49  ; loss_train =  258.91724871098995  ; loss_eval =  13.74556827545166\n",
      "iteration =  50  ; loss_train =  255.9437065422535  ; loss_eval =  12.328060150146484\n",
      "iteration =  51  ; loss_train =  271.18809373676777  ; loss_eval =  10.808147430419922\n",
      "iteration =  52  ; loss_train =  263.8550329953432  ; loss_eval =  12.604506492614746\n",
      "iteration =  53  ; loss_train =  254.61105590313673  ; loss_eval =  13.254505157470703\n",
      "iteration =  54  ; loss_train =  254.0743328332901  ; loss_eval =  13.941770553588867\n",
      "iteration =  55  ; loss_train =  250.60544776916504  ; loss_eval =  16.759532928466797\n",
      "iteration =  56  ; loss_train =  242.9824469834566  ; loss_eval =  12.656232833862305\n",
      "iteration =  57  ; loss_train =  246.1848258525133  ; loss_eval =  12.415376663208008\n",
      "iteration =  58  ; loss_train =  244.05749483406544  ; loss_eval =  10.484156608581543\n",
      "iteration =  59  ; loss_train =  238.67637540400028  ; loss_eval =  11.981734275817871\n",
      "iteration =  60  ; loss_train =  240.53938660770655  ; loss_eval =  15.04963493347168\n",
      "iteration =  61  ; loss_train =  244.92554753273726  ; loss_eval =  14.551462173461914\n",
      "iteration =  62  ; loss_train =  230.3154722005129  ; loss_eval =  13.99990177154541\n",
      "iteration =  63  ; loss_train =  222.06659671664238  ; loss_eval =  13.0588960647583\n",
      "iteration =  64  ; loss_train =  234.70702536404133  ; loss_eval =  12.254237174987793\n",
      "iteration =  65  ; loss_train =  231.07883639633656  ; loss_eval =  16.4243106842041\n",
      "iteration =  66  ; loss_train =  219.9703627154231  ; loss_eval =  17.793804168701172\n",
      "iteration =  67  ; loss_train =  230.31758098304272  ; loss_eval =  14.53642463684082\n",
      "iteration =  68  ; loss_train =  227.8413296341896  ; loss_eval =  11.010554313659668\n",
      "iteration =  69  ; loss_train =  218.06700943410397  ; loss_eval =  12.226366996765137\n",
      "iteration =  70  ; loss_train =  228.7246873229742  ; loss_eval =  11.88461971282959\n",
      "iteration =  71  ; loss_train =  222.3143628537655  ; loss_eval =  16.957778930664062\n",
      "iteration =  72  ; loss_train =  233.2915148884058  ; loss_eval =  13.645562171936035\n",
      "iteration =  73  ; loss_train =  225.96860165894032  ; loss_eval =  16.073366165161133\n",
      "iteration =  74  ; loss_train =  219.2696213722229  ; loss_eval =  12.692421913146973\n",
      "iteration =  75  ; loss_train =  227.56652264297009  ; loss_eval =  13.746821403503418\n",
      "iteration =  76  ; loss_train =  224.14877082407475  ; loss_eval =  11.05771255493164\n",
      "iteration =  77  ; loss_train =  233.68629014492035  ; loss_eval =  12.25243091583252\n",
      "iteration =  78  ; loss_train =  208.51125134527683  ; loss_eval =  15.830731391906738\n",
      "iteration =  79  ; loss_train =  222.45360979437828  ; loss_eval =  11.265303611755371\n",
      "iteration =  80  ; loss_train =  213.993676379323  ; loss_eval =  14.134956359863281\n",
      "iteration =  81  ; loss_train =  221.5668141245842  ; loss_eval =  10.74127197265625\n",
      "iteration =  82  ; loss_train =  208.2041572034359  ; loss_eval =  11.772909164428711\n",
      "iteration =  83  ; loss_train =  216.87000314891338  ; loss_eval =  12.17644214630127\n",
      "iteration =  84  ; loss_train =  213.6662568822503  ; loss_eval =  11.312772750854492\n",
      "iteration =  85  ; loss_train =  205.85416477173567  ; loss_eval =  12.379996299743652\n",
      "iteration =  86  ; loss_train =  202.23351633548737  ; loss_eval =  12.826347351074219\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_model(model, image_size, sigma_noise, 30, 529, 100, 100, nbr_inliers, nbr_outliers)\n",
    "train_model(model_old_loss, image_size, sigma_noise, 30, 529, 100, 100, nbr_inliers, nbr_outliers, loss = 0)\n",
    "train_model_patch(model_patch, image_size, sigma_noise, 30, 529, 100, 100, nbr_inliers, nbr_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test weights results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_weights_image(model1, model2, model3, image, sigma, nbr_inliers, nbr_outliers):\n",
    "    line_ref = generate_line(image)\n",
    "    data = generate_data(line_ref, image, sigma, nbr_inliers, nbr_outliers)\n",
    "    n_inliers_precise = len(data[0])-nbr_outliers\n",
    "    dist_ref = dist_points_to_line(data, line_ref, image, n_inliers_precise)\n",
    "    n = len(data[0])\n",
    "    print(n)\n",
    "    \n",
    "    for it in range(10):\n",
    "        print(\"iteration = \", it)\n",
    "        i1 = np.random.randint(n)\n",
    "        scale = 10\n",
    "        all_features_nn = [[-1]*(image[0]//scale) for _ in range(image[1]//scale)]\n",
    "        all_features_patch1 = [[-1]*(image[0]//scale) for _ in range(image[1]//scale)]\n",
    "        all_features_patch2 = [[-1]*(image[0]//scale) for _ in range(image[1]//scale)]\n",
    "        all_features_patch3 = [[-1]*(image[0]//scale) for _ in range(image[1]//scale)]\n",
    "        data[0].append(-1)\n",
    "        data[1].append(-1)\n",
    "        print(len(data[0]))\n",
    "        p1 = (data[0][i1], data[1][i1])\n",
    "        weights_ref = [[-1]*(image[0]//scale) for _ in range(image[1]//scale)]\n",
    "        weights_ref_old = [[-1]*(image[0]//scale) for _ in range(image[1]//scale)]\n",
    "        for y in range(0, image[1], scale):\n",
    "            for x in range(0, image[0], scale):\n",
    "                p2 = (x/image[0], y/image[0])\n",
    "                data[0][-1] = float(x)/image[0]\n",
    "                data[1][-1] = float(y)/image[1]\n",
    "                #print(p1)\n",
    "                #print(p2)\n",
    "                feature_nn = compute_features_k(data, i1, n)\n",
    "                all_features_nn[(image[1]-y-1)//scale][x//scale] = feature_nn\n",
    "                feature1, feature2, feature3=compute_features_patch(data, i1, n, image)\n",
    "                all_features_patch1[(image[1]-y-1)//scale][x//scale] = feature1\n",
    "                all_features_patch2[(image[1]-y-1)//scale][x//scale] = feature2\n",
    "                all_features_patch3[(image[1]-y-1)//scale][x//scale] = feature3\n",
    "\n",
    "                line = compute_line_from_points(p1, p2, image)\n",
    "                area_old = loss_weights(line_ref, line, image)\n",
    "                area = loss_dist_points(data, line, dist_ref, image, n_inliers_precise)\n",
    "                weights_ref[(image[1]-y-1)//scale][x//scale] = area\n",
    "                weights_ref_old[(image[1]-y-1)//scale][x//scale] = area_old\n",
    "        \n",
    "        del(data[0][n])\n",
    "        del(data[1][n])\n",
    "        print(len(data[0]))\n",
    "\n",
    "        all_features_nn = list(chain.from_iterable(all_features_nn))\n",
    "        all_features_patch1 = list(chain.from_iterable(all_features_patch1))\n",
    "        all_features_patch2 = list(chain.from_iterable(all_features_patch2))\n",
    "        all_features_patch3 = list(chain.from_iterable(all_features_patch3))\n",
    "        \n",
    "        model1.eval()\n",
    "        model2.eval()\n",
    "        model3.eval()\n",
    "        with torch.no_grad():                \n",
    "            X_tensor_nn = torch.Tensor(all_features_nn)\n",
    "            weights1 = model1(X_tensor_nn.to(DEVICE))\n",
    "\n",
    "            X_tensor_patch1 = torch.Tensor(all_features_patch1)\n",
    "            X_tensor_patch2 = torch.Tensor(all_features_patch2)\n",
    "            X_tensor_patch3 = torch.Tensor(all_features_patch3)\n",
    "            weights2 = model2((X_tensor_patch1.to(DEVICE).float(), X_tensor_patch2.to(DEVICE).float(), X_tensor_patch3.to(DEVICE).float()))\n",
    "\n",
    "            weights3 = model3(X_tensor_nn.to(DEVICE))\n",
    " \n",
    "        plot_first_point(data, line_ref, p1, image)\n",
    "        plot_image(weights_ref_old, image, scale)\n",
    "        plot_image(weights3.cpu(), image, scale)\n",
    "        plot_image(weights_ref, image, scale)\n",
    "        plot_image(weights1.cpu(), image, scale)\n",
    "        plot_image(weights2.cpu(), image, scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_weights_image(model, model_patch, model_old_loss, image_size, sigma_noise, nbr_inliers, nbr_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_nn(model, image, sigma, nbr_inliers, nbr_outliers):\n",
    "    line_ref = generate_line(image)\n",
    "    data = generate_data(line_ref, image, sigma, nbr_inliers, nbr_outliers)\n",
    "    n_inliers_precise = len(data[0])-nbr_outliers\n",
    "    dist_ref = dist_points_to_line(data, line_ref, image, n_inliers_precise)\n",
    "    n = len(data[0])\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    plot_data(data, ax)\n",
    "    fig.show()\n",
    "    \n",
    "    \n",
    "    for it in range(10):\n",
    "        i1 = np.random.randint(n)\n",
    "        X = []\n",
    "        coord = []\n",
    "        weights_ref = []\n",
    "        p1 = (data[0][i1], data[1][i1])\n",
    "        for i2 in range(n):\n",
    "            if i2!=i1:\n",
    "                feature=compute_features(data, i1, i2)\n",
    "                X.append(feature)\n",
    "                  \n",
    "                \n",
    "                p2 = (data[0][i2], data[1][i2])\n",
    "                line = compute_line_from_points(p1, p2, image)\n",
    "                #area = loss_weights(line_ref, line, image)\n",
    "                area = loss_dist_points(data, line, dist_ref, image, n_inliers_precise)\n",
    "                weights_ref.append([area])\n",
    "                \n",
    "                coord.append((data[0][i2], data[1][i2]))\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.Tensor(X)\n",
    "            weights = model(X_tensor)\n",
    "        \n",
    "        #plot_weights(weights, coord, p1, image)\n",
    "        plot_weights(weights_ref, coord, p1, image, ref=True)\n",
    "        \n",
    "def test_nn_compare(model1, model2, model3, image, sigma, nbr_inliers, nbr_outliers):\n",
    "    line_ref = generate_line(image)\n",
    "    data = generate_data(line_ref, image, sigma, nbr_inliers, nbr_outliers)\n",
    "    n_inliers_precise = len(data[0])-nbr_outliers\n",
    "    dist_ref = dist_points_to_line(data, line_ref, image, n_inliers_precise)\n",
    "    n = len(data[0])\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    plot_data(data, ax)\n",
    "    fig.show()\n",
    "    \n",
    "    \n",
    "    for it in range(10):\n",
    "        i1 = np.random.randint(n)\n",
    "        X_nn = []\n",
    "        X_nn_old = []\n",
    "        X_patch1 = []\n",
    "        X_patch2 = []\n",
    "        X_patch3 = []\n",
    "        coord = []\n",
    "        weights_ref = []\n",
    "        weights_ref_old = []\n",
    "        p1 = (data[0][i1], data[1][i1])\n",
    "        for i2 in range(n):\n",
    "            if i2!=i1:\n",
    "                feature_nn=compute_features_k(data, i1, i2)\n",
    "                X_nn.append(feature_nn)\n",
    "                \n",
    "                feature1, feature2, feature3=compute_features_patch(data, i1, i2, image)\n",
    "                X_patch1.append(feature1)\n",
    "                X_patch2.append(feature2)\n",
    "                X_patch3.append(feature3)\n",
    "                  \n",
    "                \n",
    "                p2 = (data[0][i2], data[1][i2])\n",
    "                line = compute_line_from_points(p1, p2, image)\n",
    "                area_old = loss_weights(line_ref, line, image)\n",
    "                area = loss_dist_points(data, line, dist_ref, image, n_inliers_precise)\n",
    "                weights_ref.append([area])\n",
    "                weights_ref_old.append([area_old])\n",
    "\n",
    "                coord.append((data[0][i2], data[1][i2]))\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():                \n",
    "            X_tensor_nn = torch.Tensor(X_nn)\n",
    "            weights1 = model1(X_tensor_nn.to(DEVICE))\n",
    "            \n",
    "            X_tensor_patch1 = torch.Tensor(X_patch1)\n",
    "            X_tensor_patch2 = torch.Tensor(X_patch2)\n",
    "            X_tensor_patch3 = torch.Tensor(X_patch3)\n",
    "            weights2 = model2((X_tensor_patch1.to(DEVICE).float(), X_tensor_patch2.to(DEVICE).float(), X_tensor_patch3.to(DEVICE).float()))\n",
    "            \n",
    "            weights3 = model3(X_tensor_nn.to(DEVICE))\n",
    "\n",
    "        \n",
    "        plot_weights(weights_ref_old, coord, p1, image)\n",
    "        plot_weights(weights3, coord, p1, image)\n",
    "        \n",
    "        plot_weights(weights_ref, coord, p1, image)\n",
    "        plot_weights(weights1, coord, p1, image)\n",
    "        plot_weights(weights2, coord, p1, image)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_nn_compare(model, model_patch, model_old_loss, image_size, sigma_noise, nbr_inliers, nbr_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ransac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inliers(data, line, threshold, image):\n",
    "    a, b = line\n",
    "    X, Y = data\n",
    "    Inliers = [[], []]\n",
    "    for i in range(len(X)):\n",
    "        (x, y) = (X[i], Y[i])\n",
    "        (xb, yb) = x*image[0], y*image[1]\n",
    "        if abs((a*xb+b) - yb)/sqrt(1+a*a) < threshold:\n",
    "            Inliers[0].append(x)\n",
    "            Inliers[1].append(y)\n",
    "    return Inliers\n",
    "\n",
    "def get_first_point(method, data, image, densities):\n",
    "    if method == 0:\n",
    "        i1 = np.random.randint(len(data[0]))\n",
    "    elif method == 1:\n",
    "        n = len(data[0])\n",
    "        i1 = sample_from_weights(data, densities)\n",
    "        \n",
    "    return i1\n",
    "        \n",
    "\n",
    "def get_model(model, model_type, data, image, type_loss, type_first_point, line_ref, densities):\n",
    "    i1 = get_first_point(type_first_point, data, image, densities)\n",
    "    if line_ref is None:\n",
    "        weights = compute_weights(model, model_type, data, i1, image)\n",
    "    else:\n",
    "        weights = compute_weights_ref(line_ref, image, data, i1, type_loss)\n",
    "    #print(mean_weights(weights))\n",
    "    i2 = sample_from_weights(data, weights)\n",
    "    p1 = data[0][i1], data[1][i1]\n",
    "    p2 = data[0][i2], data[1][i2]\n",
    "    line = compute_line_from_points(p1, p2, image)\n",
    "    return line\n",
    "\n",
    "\n",
    "def update_stopping_criterion(k, inliers, proba, it_end):\n",
    "    if k >= it_end:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def ransac(data, threshold, image, proba, type_first_point, type_loss, model_type = None, line_ref=None, model=None, compare=False, it_end=150, n_x=0.05, n_y=0.05):\n",
    "    k = 0\n",
    "    inliers_max = [[], []]\n",
    "    line_max = None\n",
    "    end = False\n",
    "    nbr_inliers = []\n",
    "    densities = get_density_vector(data, image, n_x, n_y)\n",
    "    while not end:\n",
    "        line = get_model(model, model_type, data, image, type_loss, type_first_point, line_ref, densities)\n",
    "        inliers = get_inliers(data, line, threshold, image)\n",
    "        if len(inliers[0]) > len(inliers_max[0]):\n",
    "            inliers_max = inliers\n",
    "            line_max = line\n",
    "        nbr_inliers.append(len(inliers_max[0]))\n",
    "\n",
    "        k += 1\n",
    "        end = update_stopping_criterion(k, inliers, proba, it_end)\n",
    "        \n",
    "        #print(\"------------------------------------\")\n",
    "        #print(\"nbr inliers = \", len(inliers[0]))\n",
    "        #print(\"line = \", line)\n",
    "        #print(\"max inliers so far = \", len(inliers_max[0]))\n",
    "    if compare:\n",
    "        return line_max, inliers_max, nbr_inliers\n",
    "    return line_max, inliers_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniform Ransac\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_uniform(data, image):\n",
    "    n = len(data[0])\n",
    "    i1 = np.random.randint(n)\n",
    "    i2 = np.random.randint(n)\n",
    "    while i1 == i2:\n",
    "        i2 = np.random.randint(n)\n",
    "    p1 = data[0][i1], data[1][i1]\n",
    "    p2 = data[0][i2], data[1][i2]\n",
    "    line = compute_line_from_points(p1, p2, image)\n",
    "    return line\n",
    "\n",
    "\n",
    "def ransac_uniform(data, threshold, image, proba, it_end=150):\n",
    "    k = 0\n",
    "    inliers_max = [[], []]\n",
    "    line_max = None\n",
    "    end = False\n",
    "    nbr_inliers = []\n",
    "    while not end:\n",
    "        line = get_model_uniform(data, image)\n",
    "        inliers = get_inliers(data, line, threshold, image)\n",
    "        if len(inliers[0]) > len(inliers_max[0]):\n",
    "            inliers_max = inliers\n",
    "            line_max = line\n",
    "        nbr_inliers.append(len(inliers_max[0]))\n",
    "\n",
    "        k += 1\n",
    "        end = update_stopping_criterion(k, inliers, proba, it_end)\n",
    "        \n",
    "        #print(\"------------------------------------\")\n",
    "        #print(\"nbr inliers = \", len(inliers[0]))\n",
    "        #print(\"line = \", line)\n",
    "        #print(\"max inliers so far = \", len(inliers_max[0]))\n",
    "    return line_max, inliers_max, nbr_inliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Ransacs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_uniform(image, sigma, n_inliers, n_outliers, type_loss, model=None, model_type=None,average=5):\n",
    "    Lines = []\n",
    "    Datas = []\n",
    "    for k in range(average):\n",
    "        line = generate_line(image)\n",
    "        data = generate_data(line, image, sigma, n_inliers, n_outliers)\n",
    "        Lines.append(line)\n",
    "        Datas.append(data)\n",
    "    \n",
    "    threshold = 5\n",
    "    proba = 0.1\n",
    "    \n",
    "    it = 150\n",
    "    nbr_inliers_sample = [0]*it\n",
    "    nbr_inliers_uniform = [0]*it\n",
    "    \n",
    "    for k in range(average):\n",
    "        print(k)\n",
    "        line_predict1, inliers1, nbr_inliers1 = ransac(Datas[k], threshold, image, proba, 0, type_loss, model_type=model_type, line_ref=Lines[k], model=model, compare=True, it_end=it)\n",
    "        line_predict2, inliers2, nbr_inliers2 = ransac_uniform(Datas[k], threshold, image, proba, it_end=it)\n",
    "        plot_lines(Datas[k], line_predict1, line_predict2, image)\n",
    "        \n",
    "        for j in range(it):\n",
    "            nbr_inliers_sample[j] += nbr_inliers1[j]\n",
    "            nbr_inliers_uniform[j] += nbr_inliers2[j]\n",
    "    \n",
    "    for j in range(it):\n",
    "        nbr_inliers_sample[j] /=average\n",
    "        nbr_inliers_uniform[j] /= average\n",
    "        \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(nbr_inliers_sample, c='g')\n",
    "    ax.plot(nbr_inliers_uniform, c='r')\n",
    "    fig.show()\n",
    "    \n",
    "    print(\"done\")\n",
    "\n",
    "def compare_all_ransac(image, sigma, models, n_inliers, n_outliers, average=5):\n",
    "    Lines = []\n",
    "    Datas = []\n",
    "    for k in range(average):\n",
    "        line = generate_line(image)\n",
    "        data = generate_data(line, image, sigma, n_inliers, n_outliers)\n",
    "        Lines.append(line)\n",
    "        Datas.append(data)\n",
    "    \n",
    "    threshold = 5\n",
    "    proba = 0.1\n",
    "    \n",
    "    it = 150\n",
    "    n_types = len(models)\n",
    "    nbr_inliers_all = [[0]*it for _ in range(n_types+1)]\n",
    "    colors = ['black', 'green', 'red', 'yellow', 'orange', 'blue']\n",
    "    \n",
    "    for k in range(average):\n",
    "        print(k)\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_ylim(0, 1)\n",
    "        data = Datas[k]\n",
    "        plot_data(data, ax)\n",
    "        for j in range(n_types):\n",
    "            (model, model_type, type_loss, first_point, ref) = models[j]\n",
    "            if ref:\n",
    "                line_ref=Lines[k]\n",
    "            else:\n",
    "                line_ref=None\n",
    "            line_predict, inliers, nbr_inliers = ransac(data, threshold, image, proba, first_point, type_loss, model_type = model_type, line_ref=line_ref, model=model, compare=True, it_end=it)\n",
    "            plot_line(line_predict, image, ax, c=colors[j%len(colors)])\n",
    "\n",
    "            for i in range(it):\n",
    "                nbr_inliers_all[j][i] += nbr_inliers[i]\n",
    "        \n",
    "        line_predict_uni, inliers_unie, nbr_inliers_uni = ransac_uniform(data, threshold, image, proba, it_end=it)\n",
    "        plot_line(line_predict_uni, image, ax, c=colors[n_types%len(colors)])\n",
    "        for i in range(it):\n",
    "            nbr_inliers_all[-1][i] += nbr_inliers_uni[i]\n",
    "\n",
    "    \n",
    "    for j in range(n_types+1):\n",
    "        for i in range(it):\n",
    "            nbr_inliers_all[j][i] /=average\n",
    "        \n",
    "    fig2, ax2 = plt.subplots()\n",
    "    for j in range(n_types+1):\n",
    "        ax2.plot(nbr_inliers_all[j], c=colors[j%len(colors)])\n",
    "    fig2.show()\n",
    "    \n",
    "    print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_uniform(image_size, sigma_noise, nbr_inliers, nbr_outliers, 2, model=model, model_type=1, average=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = [(None, None, 1, 0, True), (model_old_loss, 1, 1, 0, False), (None, None, 2, 0, True), (model, 1, 2, 0, False), (model_patch, 2, 2, 0, False)]\n",
    "compare_all_ransac(image_size, sigma_noise, all_models, nbr_inliers, nbr_outliers, average=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1st point : density + weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model3(model, type_model, data, image, densities, last_weights):\n",
    "    n = len(data[0])\n",
    "    \n",
    "    threshold = 0.9\n",
    "    if last_weights is not None and mean_weights(last_weights) >= threshold:\n",
    "        i1 = sample_from_weights(data, last_weights)\n",
    "    else:\n",
    "        i1 = sample_from_weights(data, densities)\n",
    "    i1 = np.random.randint(n)    \n",
    "    weights = compute_weights(model, type_model, data, i1, image)\n",
    "    #print(mean_weights(weights))\n",
    "    i2 = sample_from_weights(data, weights)\n",
    "    p1 = data[0][i1], data[1][i1]\n",
    "    p2 = data[0][i2], data[1][i2]\n",
    "    line = compute_line_from_points(p1, p2, image)\n",
    "    return line, weights\n",
    "\n",
    "\n",
    "def ransac3(data, threshold, image, proba, model, type_model, n_x, n_y, compare=False, it_end=150):\n",
    "    k = 0\n",
    "    inliers_max = [[], []]\n",
    "    line_max = None\n",
    "    end = False\n",
    "    nbr_inliers = []\n",
    "    densities = get_density_vector(data, image, n_x, n_y)\n",
    "    previous_weights = None\n",
    "    while not end:\n",
    "        line, previous_weights = get_model3(model, type_model, data, image, densities, previous_weights)\n",
    "        inliers = get_inliers(data, line, threshold, image)\n",
    "        if len(inliers[0]) > len(inliers_max[0]):\n",
    "            inliers_max = inliers\n",
    "            line_max = line\n",
    "        nbr_inliers.append(len(inliers_max[0]))\n",
    "\n",
    "        k += 1\n",
    "        end = update_stopping_criterion(k, inliers, proba, it_end)\n",
    "        \n",
    "        #print(\"------------------------------------\")\n",
    "        #print(\"nbr inliers = \", len(inliers[0]))\n",
    "        #print(\"line = \", line)\n",
    "        #print(\"max inliers so far = \", len(inliers_max[0]))\n",
    "    if compare:\n",
    "        return line_max, inliers_max, nbr_inliers\n",
    "    return line_max, inliers_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test to set threshold\n",
    "\n",
    "line = generate_line(image_size)\n",
    "data = generate_data(line, image_size, sigma_noise, nbr_inliers, nbr_outliers)\n",
    "threshold = 5\n",
    "proba = 0.1\n",
    "ransac3(data, threshold, image_size, proba, model, 1, n_x=0.05, n_y=0.05, compare=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopping_criterion_multi(step, inliers, total, proba):\n",
    "    if step < 5000:\n",
    "        return False\n",
    "    \n",
    "    end = True\n",
    "    for n_inliers in inliers:\n",
    "        x = 1 - (n_inliers/total)**2\n",
    "        if x**step > proba:\n",
    "            end = False\n",
    "    return end\n",
    "\n",
    "def ransac_uniform_multi(data, threshold, threshold_nbr_inliers, threshold_intersection, image, proba):\n",
    "    step = 0\n",
    "    inliers_max = []\n",
    "    nbr_inliers_max = []\n",
    "    line_max = []\n",
    "    end = False\n",
    "    nbr_inliers = []\n",
    "    n = len(data[0])\n",
    "    counter = 0\n",
    "    \n",
    "    while not end:\n",
    "        if (step % 1000 == 0):\n",
    "            print(\"step = \", step)\n",
    "            print(\"counter = \", counter)\n",
    "        line = get_model_uniform(data, image)\n",
    "        inliers = get_inliers_couple(data, line, threshold, image)\n",
    "        if len(inliers) > threshold_nbr_inliers:\n",
    "            counter += 1\n",
    "            max_inter = -1\n",
    "            max_inliers_inter = threshold_intersection - 1\n",
    "            for k in range(len(inliers_max)):\n",
    "                inter = nbr_intersection(inliers, inliers_max[k])\n",
    "                #print(\"intersection = \", inter)\n",
    "                if inter > max_inliers_inter:\n",
    "                    #print(len(inliers), len(inliers_max[k]))\n",
    "                    #print(sorted(inliers))\n",
    "                    #print()\n",
    "                    #print(sorted(inliers_max[k]))\n",
    "                    max_inliers_inter = inter\n",
    "                    max_inter = k\n",
    "                    \n",
    "            if max_inter == -1:\n",
    "                print(\"new line\")\n",
    "                line_max.append(line)\n",
    "                inliers_max.append(inliers)\n",
    "                nbr_inliers_max.append(len(inliers))\n",
    "            else:\n",
    "                #print(\"change line\", max_inter, \",\", \"intersection =\", max_inliers_inter)\n",
    "                if len(inliers) > nbr_inliers_max[max_inter]:\n",
    "                    line_max[max_inter] = line\n",
    "                    inliers_max[max_inter] = inliers\n",
    "                    nbr_inliers_max[max_inter] = len(inliers)\n",
    "\n",
    "        step += 1\n",
    "        end = stopping_criterion_multi(step, nbr_inliers_max, n, proba)\n",
    "        \n",
    "        #print(\"------------------------------------\")\n",
    "        #print(\"nbr inliers = \", len(inliers))\n",
    "        #print(\"line = \", line)\n",
    "        #print(\"max inliers so far = \", nbr_inliers_max)\n",
    "    return line_max, inliers_max, nbr_inliers_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image_size = (200, 200)\n",
    "#n_inliers = 250\n",
    "#n_outliers = 400\n",
    "#sigma_noise = (2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line1 = generate_line(image_size)\n",
    "line2 = generate_line(image_size)\n",
    "data = generate_data_multi_line([line1, line2], image_size, sigma_noise, nbr_inliers, nbr_outliers)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "plot_data(data, ax)\n",
    "plot_line(line1, image_size, ax)\n",
    "plot_line(line2, image_size, ax)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba = 0.05\n",
    "threshold = 2.5\n",
    "threshold_nbr_inliers = 100\n",
    "threshold_intersection = 20\n",
    "\n",
    "lines_pred, inliers_pred, nbr_inliers_pred = ransac_uniform_multi(data, threshold, threshold_nbr_inliers, threshold_intersection, image_size, proba)\n",
    "print(\"nbr_lines = \", len(lines_pred))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "plot_data(data, ax, c='b')\n",
    "colors = ['r', 'g', 'b']\n",
    "for i in range(len(inliers_pred)):\n",
    "    plot_data(change_format(inliers_pred[i]), ax, c=colors[i % 3])\n",
    "    plot_line(lines_pred[i], image_size, ax)\n",
    "\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Improved Ransac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ransac_multi_improved(data, threshold, threshold_nbr_inliers, threshold_intersection, image, proba, type_first_point, type_loss, model_type = None, line_ref=None, model=None, compare=False, it_end=150, n_x=0.05, n_y=0.05):\n",
    "    step = 0\n",
    "    inliers_max = []\n",
    "    nbr_inliers_max = []\n",
    "    line_max = []\n",
    "    end = False\n",
    "    nbr_inliers = []\n",
    "    n = len(data[0])\n",
    "    counter = 0\n",
    "    \n",
    "    densities = get_density_vector(data, image, n_x, n_y)\n",
    "    while not end:\n",
    "        if (step % 10 == 0):\n",
    "            print(\"step = \", step)\n",
    "            print(\"counter = \", counter)\n",
    "        line = get_model(model, model_type, data, image, type_loss, type_first_point, line_ref, densities)\n",
    "        inliers = get_inliers_couple(data, line, threshold, image)\n",
    "        print(len(inliers))\n",
    "        if len(inliers) > threshold_nbr_inliers:\n",
    "            counter += 1\n",
    "            max_inter = -1\n",
    "            max_inliers_inter = threshold_intersection*len(inliers) - 1\n",
    "            for k in range(len(inliers_max)):\n",
    "                inter = nbr_intersection(inliers, inliers_max[k])\n",
    "                #print(\"intersection = \", inter)\n",
    "                if inter > max_inliers_inter:\n",
    "                    #print(len(inliers), len(inliers_max[k]))\n",
    "                    #print(sorted(inliers))\n",
    "                    #print()\n",
    "                    #print(sorted(inliers_max[k]))\n",
    "                    max_inliers_inter = inter\n",
    "                    max_inter = k\n",
    "                    \n",
    "            if max_inter == -1:\n",
    "                print(\"new line\")\n",
    "                line_max.append(line)\n",
    "                inliers_max.append(inliers)\n",
    "                nbr_inliers_max.append(len(inliers))\n",
    "            else:\n",
    "                print(\"change line\", max_inter, \",\", \"intersection =\", max_inliers_inter)\n",
    "                if len(inliers) > nbr_inliers_max[max_inter]:\n",
    "                    line_max[max_inter] = line\n",
    "                    inliers_max[max_inter] = inliers\n",
    "                    nbr_inliers_max[max_inter] = len(inliers)\n",
    "\n",
    "        step += 1\n",
    "        #end = stopping_criterion_multi(step, nbr_inliers_max, n, proba)\n",
    "        end = (step > it_end)\n",
    "        \n",
    "        #print(\"------------------------------------\")\n",
    "        #print(\"nbr inliers = \", len(inliers))\n",
    "        #print(\"line = \", line)\n",
    "        #print(\"max inliers so far = \", nbr_inliers_max)\n",
    "    return line_max, inliers_max, nbr_inliers_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba = 0.05\n",
    "threshold = 2.5\n",
    "threshold_nbr_inliers = 100\n",
    "threshold_intersection = 0.2\n",
    "\n",
    "lines_pred, inliers_pred, nbr_inliers_pred = ransac_multi_improved(data, threshold, threshold_nbr_inliers, threshold_intersection, image_size, proba, 0, 1, model_type=1, model=model, it_end=100)\n",
    "print(\"nbr_lines = \", len(lines_pred))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "plot_data(data, ax, c='b')\n",
    "colors = ['r', 'g', 'b']\n",
    "for i in range(len(inliers_pred)):\n",
    "    plot_data(change_format(inliers_pred[i]), ax, c=colors[i % 3])\n",
    "    plot_line(lines_pred[i], image_size, ax)\n",
    "\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line1 = generate_line(image_size)\n",
    "line2 = generate_line(image_size)\n",
    "data = generate_data_multi_line([line1, line2], image_size, sigma_noise, nbr_inliers, nbr_outliers)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "plot_data(data, ax)\n",
    "plot_line(line1, image_size, ax)\n",
    "plot_line(line2, image_size, ax)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
